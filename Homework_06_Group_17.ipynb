{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cddc259",
   "metadata": {
    "id": "4cddc259"
   },
   "source": [
    "# IANNWTF - Homework 06\n",
    "(by Group 17 - Nils Niehaus, Philipp Bauer, Marlon Dammann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c5043",
   "metadata": {
    "id": "6f6c5043"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a7942",
   "metadata": {
    "id": "9f5a7942"
   },
   "source": [
    "## 1 Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8RylNMcy4bns",
   "metadata": {
    "id": "8RylNMcy4bns"
   },
   "source": [
    "We load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084eee3",
   "metadata": {
    "id": "d084eee3"
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4af024",
   "metadata": {
    "id": "3e4af024"
   },
   "source": [
    "and apply the appropriate preprocessing including normalizing, shuffling, prefetching and batching the data as well as onehotifying the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b104a1",
   "metadata": {
    "id": "b3b104a1"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(data, target):\n",
    "    '''We combine the image and target into one dataset'''\n",
    "    data = tf.data.Dataset.from_tensor_slices((data, target))\n",
    "    '''We one-hotify the target and cast into float32 for target and img'''\n",
    "    data = data.map(lambda img, target: (tf.cast(img, 'float32'), tf.one_hot(target[0],depth=10, dtype='float32')))\n",
    "    '''We normalize the color-values to a range of 0-1'''  \n",
    "    data = data.map(lambda img, target: (img/255., target))  \n",
    "    '''We shuffle, take a batch size of 64 and prefetch 20 elements.'''\n",
    "    data = data.shuffle(1000)\n",
    "    data = data.batch(64)\n",
    "    data = data.prefetch(20)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752da607",
   "metadata": {
    "id": "752da607"
   },
   "outputs": [],
   "source": [
    "train_ds = preprocess_dataset(x_train, y_train)\n",
    "test_ds = preprocess_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77b564",
   "metadata": {
    "id": "2c77b564"
   },
   "source": [
    "## 2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64d9ef",
   "metadata": {
    "id": "4a64d9ef"
   },
   "source": [
    "# ResNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfa55a",
   "metadata": {
    "id": "c2cfa55a"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, filters_in=64, filters_out=256, mode='normal'):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.ReLu = tf.keras.layers.Activation(tf.nn.relu)\n",
    "        if mode == 'normal':\n",
    "            self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv1 = tf.keras.layers.Conv2D(filters=filters_in,kernel_size=(1,1),padding='same')\n",
    "            self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv2 = tf.keras.layers.Conv2D(filters=filters_in,kernel_size=(3,3),padding='same')\n",
    "            self.batchnorm3 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv3 = tf.keras.layers.Conv2D(filters=filters_out,kernel_size=(1,1),padding='same')\n",
    "            \n",
    "            self.conv4 = tf.keras.layers.Conv2D(filters=filters_out,kernel_size=(1,1),padding='same')\n",
    "            \n",
    "            self.add = tf.keras.layers.Add()\n",
    "        elif mode == 'strided':\n",
    "            self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv1 = tf.keras.layers.Conv2D(filters=filters_in,kernel_size=(1,1),padding='same')\n",
    "            self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv2 = tf.keras.layers.Conv2D(filters=filters_in,kernel_size=(3,3),padding='same',strides=(2,2))\n",
    "            self.batchnorm3 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv3 = tf.keras.layers.Conv2D(filters=256,kernel_size=(1,1),padding='same')\n",
    "            \n",
    "            self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(1,1),strides=(2,2))\n",
    "            \n",
    "            self.add = tf.keras.layers.Add()\n",
    "        elif mode == 'constant':\n",
    "            self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv1 = tf.keras.layers.Conv2D(filters=filters_in,kernel_size=(1,1),padding='same')\n",
    "            self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv2 = tf.keras.layers.Conv2D(filters=filters_in,kernel_size=(3,3),padding='same')\n",
    "            self.batchnorm3 = tf.keras.layers.BatchNormalization()\n",
    "            self.conv3 = tf.keras.layers.Conv2D(filters=filters_out,kernel_size=(1,1),padding='same')\n",
    "    \n",
    "    def call(self, x, is_train=False):\n",
    "        if self.mode == 'normal':\n",
    "            x1 = self.batchnorm1(x,training=is_train)\n",
    "            x1 = self.ReLu(x1)\n",
    "            x1 = self.conv1(x1)\n",
    "            x1 = self.batchnorm2(x1,training=is_train)\n",
    "            x1 = self.ReLu(x1)\n",
    "            x1 = self.conv2(x1)\n",
    "            x1 = self.batchnorm3(x1,training=is_train)\n",
    "            x1 = self.ReLu(x1)\n",
    "            x1 = self.conv3(x1)\n",
    "            \n",
    "            x2 = self.conv4(x)\n",
    "            \n",
    "            x = self.add([x1, x2])\n",
    "        elif self.mode == 'strided':\n",
    "            x1 = self.batchnorm1(x,training=is_train)\n",
    "            x1 = self.ReLu(x1)\n",
    "            x1 = self.conv1(x1)\n",
    "            x1 = self.batchnorm2(x1,training=is_train)\n",
    "            x1 = self.ReLu(x1)\n",
    "            x1 = self.conv2(x1)\n",
    "            x1 = self.batchnorm3(x1,training=is_train)\n",
    "            x1 = self.ReLu(x1)\n",
    "            x1 = self.conv3(x1)\n",
    "            \n",
    "            x2 = self.maxpool(x)\n",
    "            \n",
    "            x = self.add([x1, x2])\n",
    "        elif self.mode == 'constant':\n",
    "            x = self.batchnorm1(x,training=is_train)\n",
    "            x = self.ReLu(x)\n",
    "            x = self.conv1(x)\n",
    "            x = self.batchnorm2(x,training=is_train)\n",
    "            x = self.ReLu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.batchnorm3(x,training=is_train)\n",
    "            x = self.ReLu(x)\n",
    "            x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1925c",
   "metadata": {
    "id": "5bb1925c"
   },
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu')\n",
    "        self.res_block1 = ResidualBlock(filters_in=64)\n",
    "        self.res_block2 = ResidualBlock(filters_in=128,mode='strided')\n",
    "        self.res_block3 = ResidualBlock(filters_in=256,mode='constant')\n",
    "        self.glavgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.classifier = tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "    \n",
    "    def call(self, x, is_train=False):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_block1(x,is_train=is_train)\n",
    "        x = self.res_block2(x,is_train=is_train)\n",
    "        x = self.res_block3(x,is_train=is_train)\n",
    "        x = self.glavgpool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5780238",
   "metadata": {
    "id": "b5780238"
   },
   "source": [
    "# DenseNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64326f6",
   "metadata": {
    "id": "b64326f6"
   },
   "outputs": [],
   "source": [
    "class TransitionLayers(tf.keras.Model):\n",
    "    def __init__(self, filters_in=64):\n",
    "        super(TransitionLayers, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=filters_in//2,kernel_size=(1,1),padding='valid',use_bias=False)\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.ReLu = tf.keras.layers.Activation(tf.nn.relu)\n",
    "        self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=(2,2),strides=(2,2),padding='valid')\n",
    "    \n",
    "    def call(self, x, is_train=False):\n",
    "       # x = self.conv1(x)\n",
    "        x = self.batchnorm(x,training=is_train)\n",
    "        x = self.ReLu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e898f5",
   "metadata": {
    "id": "73e898f5"
   },
   "outputs": [],
   "source": [
    "class DenseBlock(tf.keras.Model):\n",
    "    def __init__(self, filters_in=128, filters_out=32):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.ReLu = tf.keras.layers.Activation(tf.nn.relu)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=filters_in,kernel_size=(1,1),padding='valid',use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=filters_out,kernel_size=(3,3),padding='same',use_bias=False)\n",
    "    \n",
    "    def call(self, x, is_train=False):\n",
    "        x1 = self.bn1(x,training=is_train)\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.bn2(x1,training=is_train)\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x = tf.keras.layers.Concatenate(axis=3)([x,x1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da47145",
   "metadata": {
    "id": "9da47145"
   },
   "outputs": [],
   "source": [
    "class DenseNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu')\n",
    "        self.dense_block6 = [DenseBlock() for _ in range(6)]\n",
    "        self.transition1 = TransitionLayers(filters_in=32*7)\n",
    "        output_shape = (32*7)//2\n",
    "        self.dense_block12 = [DenseBlock() for _ in range(12)]\n",
    "        self.transition2 = TransitionLayers(filters_in=output_shape+32*12)\n",
    "        output_shape = (output_shape+32*12)//2\n",
    "        self.dense_block24 = [DenseBlock() for _ in range(24)]\n",
    "        self.transition3 = TransitionLayers(filters_in=output_shape+32*24)\n",
    "        self.dense_block16 = [DenseBlock() for _ in range(16)]\n",
    "        self.ReLu = tf.keras.layers.Activation(tf.nn.relu)\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.glavgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.classifier = tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "    \n",
    "    def call(self, x, is_train=False):\n",
    "        x = self.input_layer(x)\n",
    "        for i in range(len(self.dense_block6)):\n",
    "            x = self.dense_block6[i](x,training=is_train)\n",
    "        x = self.transition1(x,training=is_train)\n",
    "        for i in range(len(self.dense_block12)):\n",
    "            x = self.dense_block12[i](x,training=is_train)\n",
    "        x = self.transition2(x,training=is_train)\n",
    "        for i in range(len(self.dense_block24)):\n",
    "            x = self.dense_block24[i](x,training=is_train)\n",
    "        x = self.transition3(x,training=is_train)\n",
    "        for i in range(len(self.dense_block16)):\n",
    "            x = self.dense_block16[i](x,training=is_train)\n",
    "        x = self.bn(x)\n",
    "        x = self.ReLu(x)\n",
    "        x = self.glavgpool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3799fd",
   "metadata": {
    "id": "8e3799fd"
   },
   "source": [
    "## 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948e883",
   "metadata": {
    "id": "d948e883"
   },
   "outputs": [],
   "source": [
    "model_performance = {\n",
    "    'ResNet' : {\n",
    "        'training': {'loss' : [], 'accuracy' : []},\n",
    "        'test': {'loss' : [], 'accuracy' : []}\n",
    "    },\n",
    "    'DenseNet' : {\n",
    "        'training': {'loss' : [], 'accuracy' : []},\n",
    "        'test': {'loss' : [], 'accuracy' : []}\n",
    "                 }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e585d",
   "metadata": {
    "id": "2a9e585d"
   },
   "outputs": [],
   "source": [
    "dataset = {'training' : train_ds, 'test' : test_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19548485",
   "metadata": {
    "id": "19548485"
   },
   "outputs": [],
   "source": [
    "def get_loss_and_accuracy(model, data_split, loss_function, mode_name):\n",
    "    '''Returns the loss and accuracy of the model on a given split of the data.'''\n",
    "    accuracy_aggregator = []\n",
    "    loss_aggregator = []\n",
    "\n",
    "    for (input, target) in tqdm(data_split,desc='Sampling Loss/Accuracy for ' + str(mode_name) + ' data'):\n",
    "        prediction = model(input)\n",
    "        sample_loss = loss_function(target, prediction)\n",
    "        sample_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "        loss_aggregator.append(sample_loss.numpy())\n",
    "        accuracy_aggregator.append(np.mean(sample_accuracy))\n",
    "        \n",
    "    loss = tf.reduce_mean(loss_aggregator)\n",
    "    accuracy = tf.reduce_mean(accuracy_aggregator)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef3105",
   "metadata": {
    "id": "56ef3105"
   },
   "outputs": [],
   "source": [
    "def performance_test(model, data_split, loss_function, model_performance, mode_name, model_type):\n",
    "    '''Evaluation of loss and accuracy of a model on a data split.'''\n",
    "    loss, accuracy = get_loss_and_accuracy(model, data_split, loss_function, mode_name)\n",
    "    model_performance[model_type][mode_name]['loss'].append(loss.numpy())\n",
    "    model_performance[model_type][mode_name]['accuracy'].append(accuracy.numpy())\n",
    "    if mode_name == 'test':\n",
    "      print('Test accuracy for current epoch: ', model_performance[model_type][mode_name]['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9251f",
   "metadata": {
    "id": "91a9251f"
   },
   "outputs": [],
   "source": [
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input, is_train=True)\n",
    "        loss = loss_function(target, prediction)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fcb097",
   "metadata": {
    "id": "82fcb097"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataset, optimizer, loss_function, num_epochs, model_performance, model_type):\n",
    "    for data, name in zip(list(dataset.values()),list(dataset.keys())):\n",
    "        performance_test(model, data, loss_function, model_performance, mode_name=name, model_type=model_type)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for input,target in tqdm(dataset['training'],desc='Epoch '+str(epoch+1)):\n",
    "            train_step(model, input, target, loss_function, optimizer)\n",
    "            \n",
    "        for data, name in zip(list(dataset.values()),list(dataset.keys())):\n",
    "            performance_test(model, data, loss_function, model_performance, mode_name=name, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4882b16",
   "metadata": {
    "id": "b4882b16"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "'''Hyperparameters'''\n",
    "\n",
    "'''The task was to use 30 epochs but we decided that it takes way too much time and does not change that much on our result. So we plotted with 10.'''\n",
    "num_epochs = 10 \n",
    "learning_rate = 0.001\n",
    "\n",
    "'''Loss function'''\n",
    "cat_cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "'''Adam as chosen optimizer'''\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "'''Models'''\n",
    "resnet = ResNet()\n",
    "densenet = DenseNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509974e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2509974e",
    "outputId": "ef6a0e73-a770-41e5-f03b-59197b268968"
   },
   "outputs": [],
   "source": [
    "print('**Starting training for ResNet**')\n",
    "train_model(resnet, dataset, optimizer, cat_cross_entropy_loss, num_epochs, model_performance, 'ResNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7bcb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "dfc7bcb3",
    "outputId": "6272a32e-5fbf-4036-cc2f-54a7d88a9373"
   },
   "outputs": [],
   "source": [
    "print('**Starting training for DenseNet**')\n",
    "train_model(densenet, dataset, optimizer, cat_cross_entropy_loss, num_epochs, model_performance, 'DenseNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DyUrrf6aWQt2",
   "metadata": {
    "id": "DyUrrf6aWQt2"
   },
   "source": [
    "## 4 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb5bf2",
   "metadata": {
    "id": "7bfb5bf2"
   },
   "source": [
    "Plotting the accuarcy and loss results via matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c0385",
   "metadata": {
    "id": "9f0c0385"
   },
   "outputs": [],
   "source": [
    "def visualize_performance(model_type):\n",
    "    fig, ax = plt.subplots(1,2,figsize=(15, 4))\n",
    "    line1, = ax[0].plot(model_performance[model_type]['training']['loss'])\n",
    "    line2, = ax[0].plot(model_performance[model_type]['test']['loss'])\n",
    "    line3, = ax[1].plot(model_performance[model_type]['training']['accuracy'])\n",
    "    line4, = ax[1].plot(model_performance[model_type]['test']['accuracy'])\n",
    "    ax[0].set_xlabel(\"Training epoch\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[1].set_xlabel(\"Training epoch\")\n",
    "    ax[1].set_ylabel(\"Accuracy\")\n",
    "    ax[0].legend((line1,line2),(\"training\",\"test\"))\n",
    "    ax[1].legend((line3,line4),(\"training\",\"test\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9610466",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "e9610466",
    "outputId": "badceadb-3d5f-4534-fb50-29cbc1ad973f"
   },
   "outputs": [],
   "source": [
    "visualize_performance('ResNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae813b5e",
   "metadata": {
    "id": "ae813b5e"
   },
   "outputs": [],
   "source": [
    "print(model_performance['ResNet']['test']['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3e6bc",
   "metadata": {
    "id": "bba3e6bc"
   },
   "outputs": [],
   "source": [
    "visualize_performance('DenseNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KAi0jui_8D81",
   "metadata": {
    "id": "KAi0jui_8D81"
   },
   "outputs": [],
   "source": [
    "print(model_performance['DenseNet']['test']['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RwvXAwROFmm6",
   "metadata": {
    "id": "RwvXAwROFmm6"
   },
   "outputs": [],
   "source": [
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c8162",
   "metadata": {
    "id": "dd6c8162"
   },
   "outputs": [],
   "source": [
    "densenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AuQrzubO2giS",
   "metadata": {
    "id": "AuQrzubO2giS"
   },
   "source": [
    "With both resnet and densenet models being more advanced than the simple CNN architecture from last weeks homework, they have to deal with way more parameters (around 120000 compared to around 1 million and 6 million respectively). This results is significantly more time neeed for training but rather similar performance on the given dataset with the densenet and somewhat worse performance with the resnet architecture which tends to overfit and fluctuating accuracy and loss results after a few epochs(ignoring the different batchsizes)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_06_Group_17.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
